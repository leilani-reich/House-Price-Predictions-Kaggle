{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f770b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: (1460, 80)\n",
      "y_train: (1460,)\n",
      "df_test: (1459, 80)\n",
      "Data has been loaded\n"
     ]
    }
   ],
   "source": [
    "# Load in Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "y_train = df_train['SalePrice'].to_numpy()\n",
    "df_train = df_train.drop('SalePrice', 1)\n",
    "id_train = df_train['Id'].to_numpy()\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "id_test = df_test['Id'].to_numpy()\n",
    "\n",
    "print('df_train:', df_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('df_test:', df_test.shape)\n",
    "\n",
    "print('Data has been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919e2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing data done\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "num_cols = list(df_train._get_numeric_data().columns)\n",
    "cat_cols=list(set(df_train.columns) - set(num_cols))\n",
    "\n",
    "# Imputation\n",
    "\n",
    "# From a post by AJ Welch here\n",
    "# https://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe/\n",
    "# I got the df.isnull().sum().sum() part of the code below\n",
    "\n",
    "#print('Null values train:', df_train.isnull().sum().sum())\n",
    "#print('Null values test:', df_test.isnull().sum().sum())\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer.fit(df_train[cat_cols])\n",
    "\n",
    "df_train[cat_cols] = imputer.transform(df_train[cat_cols])\n",
    "df_test[cat_cols] = imputer.transform(df_test[cat_cols])\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(df_train[num_cols])\n",
    "df_train[num_cols] = imputer.transform(df_train[num_cols])\n",
    "df_test[num_cols] = imputer.transform(df_test[num_cols])\n",
    "\n",
    "# I tried iterative imputer for my numeric data in other submissions\n",
    "'''\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# try different imputation_orders\n",
    "imputer = IterativeImputer(missing_values=np.nan, initial_strategy='mean', imputation_order='random')\n",
    "imputer.fit(df_train[num_cols])\n",
    "\n",
    "df_train[num_cols] = imputer.transform(df_train[num_cols])\n",
    "df_test[num_cols] = imputer.transform(df_test[num_cols])\n",
    "'''\n",
    "\n",
    "print('Imputing missing data done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adeed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols: 43\n",
      "Numerical cols: 37\n",
      "Encoding done\n"
     ]
    }
   ],
   "source": [
    "# Encoding\n",
    "\n",
    "# Encoding - Ordinal\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_cols_dict = {'Street': ['Grvl', 'Pave'],\n",
    "                'Alley': ['Grvl', 'Pave'],\n",
    "                'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],\n",
    "                'LandContour': ['Low', 'HLS', 'Bnk', 'Lvl'],\n",
    "                'Utilities': ['ELO', 'NoSeWa', 'NoSewr', 'AllPub'],\n",
    "                'LandSlope': ['Sev', 'Mod', 'Gtl'],\n",
    "                'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'BsmtQual': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'BsmtCond': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'BsmtExposure': ['NA', 'No', 'Mn', 'Av', 'Gd'], \n",
    "                'BsmtFinType1': ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "                'BsmtFinType2': ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "                'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'CentralAir': ['N', 'Y'],\n",
    "                'Electrical': ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr'],\n",
    "                'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n",
    "                'FireplaceQu': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'GarageType': ['NA', 'Detchd', 'CarPort', 'BuiltIn', 'Basment', 'Attchd', '2Types'],\n",
    "                'GarageFinish': ['NA', 'Unf', 'RFn', 'Fin'],\n",
    "                'GarageQual': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'GarageCond': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'PavedDrive': ['N', 'P', 'Y'],\n",
    "                'PoolQC': ['NA', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                'Fence': ['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']\n",
    "               }\n",
    "\n",
    "ordinal_cols = ordinal_cols_dict.keys() # list of columns which I'm ordinal encoding\n",
    "\n",
    "cats = list(ordinal_cols_dict.values()) # Categories corresponding to each column which I'm ordinal encoding\n",
    "\n",
    "print('Categorical cols:', len(cat_cols))\n",
    "print('Numerical cols:', len(num_cols))\n",
    "\n",
    "ord_encoder = OrdinalEncoder(categories=cats)\n",
    "ordinal_train = ord_encoder.fit_transform(df_train[ordinal_cols])\n",
    "ordinal_test = ord_encoder.transform(df_test[ordinal_cols])\n",
    "\n",
    "\n",
    "# Encoding - Onehot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_cols = list(set(cat_cols) - set(ordinal_cols))\n",
    "\n",
    "onehot = OneHotEncoder(sparse=False) # handle_unknown='ignore'\n",
    "onehot.fit(df_train[onehot_cols])\n",
    "onehot_train = onehot.transform(df_train[onehot_cols])\n",
    "onehot_test = onehot.transform(df_test[onehot_cols])\n",
    "\n",
    "print('Encoding done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48ad15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "# Making datasets\n",
    "\n",
    "# Making X\n",
    "X_train = np.concatenate([df_train[num_cols], ordinal_train, onehot_train], axis=1)\n",
    "X_test = np.concatenate([df_test[num_cols], ordinal_test, onehot_test], axis=1)\n",
    "\n",
    "# Normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "normalizer.fit(X_train)\n",
    "X_train = normalizer.transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Other data transformation techniques tried:\n",
    "\n",
    "# PowerTransformer (log transform)\n",
    "'''\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "log = PowerTransformer()\n",
    "log.fit(X_train)\n",
    "X_train = log.transform(X_train)\n",
    "X_test = log.transform(X_test)\n",
    "'''\n",
    "\n",
    "# Standardization\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Shuffling data is always good, especially since I'm making a val set\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "\n",
    "# Making additional validation set to evaluate performance beforehand\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "print('\\nX_train shape:', X_train.shape)\n",
    "print(' X_test shape:', X_test.shape)\n",
    "print('  X_val shape:', X_val.shape)\n",
    "print('  y_val shape:', y_val.shape)\n",
    "'''\n",
    "\n",
    "print(\"Preprocessing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081fb46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared log error (RMSLE) custom scorer done\n"
     ]
    }
   ],
   "source": [
    "# Making root mean squared log error scorer for my models\n",
    "# since Kaggle is using this metric for evaluation\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def log_rmsle(pred, true):\n",
    "    return -np.sqrt(np.mean(np.square((np.log(pred + 1) - np.log(true + 1)))))\n",
    "\n",
    "RMSLE = make_scorer(log_rmsle, greater_is_better=False)\n",
    "print('Root mean squared log error (RMSLE) custom scorer done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b4c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\nparam_grid = {\\n    \\'n_estimators\\': range(1, 300, 10),\\n    \\'max_depth\\': range(1, 300, 1),\\n    \\'min_samples_split\\': np.arange(0.01, 1.01, 0.01),\\n    \\'min_samples_leaf\\': range(1, 300, 1),\\n    \\'max_leaf_nodes\\': range(1, 300, 1),\\n    \\'min_impurity_decrease\\': np.arange(0.01, 1.01, 0.01)\\n}\\n\\nfor i in range(5):\\n    random_search_RFR = RandomizedSearchCV(estimator = RandomForestRegressor(random_state=0), \\n                                 param_distributions = param_grid, cv = 5, n_jobs = -1,\\n                                 verbose = 2, scoring=RMSLE, error_score=\\'raise\\')\\n\\n    random_search_RFR.fit(X_train, y_train)\\n\\n    print(f\\'Randomized Search Results {i}:\\')\\n    print(\\'Best parameters:\\', random_search_RFR.best_params_)\\n    print(\"Lowest RMSLE: \", random_search_RFR.best_score_, \\'\\n\\n\\')\\n\\n# If chosen as final model\\n\\np = random_search_RFR.best_params_\\n\\nbest_model_RFR = RandomForestRegressor(n_estimators=p[\\'n_estimators\\'], max_depth=p[\\'max_depth\\'],\\n                                   min_samples_split=p[\\'min_samples_split\\'],\\n                                   min_samples_leaf=p[\\'min_samples_leaf\\'],\\n                                   max_leaf_nodes=p[\\'max_leaf_nodes\\'],\\n                                   min_impurity_decrease=p[\\'min_impurity_decrease\\'])\\n\\nbest_model_RFR.fit(X_train, y_train)\\n\\ny_pred = best_model_RFR.predict(X_val)\\n\\nfrom sklearn.metrics import mean_squared_error\\nprint(\\'\\nRMSE for validation set:\\', mean_squared_error(y_val, y_pred, squared=False))\\nprint(\\'\\nRMSE for log of validation set:\\', mean_squared_error(np.log(y_val), np.log(y_pred), squared=False))\\n\\n\\nprint(\\'Random Forest Model done\\')\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForestRegressor (NOT USING)\n",
    "'''\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': range(1, 300, 10),\n",
    "    'max_depth': range(1, 300, 1),\n",
    "    'min_samples_split': np.arange(0.01, 1.01, 0.01),\n",
    "    'min_samples_leaf': range(1, 300, 1),\n",
    "    'max_leaf_nodes': range(1, 300, 1),\n",
    "    'min_impurity_decrease': np.arange(0.01, 1.01, 0.01)\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    random_search_RFR = RandomizedSearchCV(estimator = RandomForestRegressor(random_state=0), \n",
    "                                 param_distributions = param_grid, cv = 5, n_jobs = -1,\n",
    "                                 verbose = 2, scoring=RMSLE, error_score='raise')\n",
    "\n",
    "    random_search_RFR.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Randomized Search Results {i}:')\n",
    "    print('Best parameters:', random_search_RFR.best_params_)\n",
    "    print(\"Lowest RMSLE: \", random_search_RFR.best_score_, '\\n\\n')\n",
    "\n",
    "# If chosen as final model\n",
    "\n",
    "p = random_search_RFR.best_params_\n",
    "\n",
    "best_model_RFR = RandomForestRegressor(n_estimators=p['n_estimators'], max_depth=p['max_depth'],\n",
    "                                   min_samples_split=p['min_samples_split'],\n",
    "                                   min_samples_leaf=p['min_samples_leaf'],\n",
    "                                   max_leaf_nodes=p['max_leaf_nodes'],\n",
    "                                   min_impurity_decrease=p['min_impurity_decrease'])\n",
    "\n",
    "best_model_RFR.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model_RFR.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('\\nRMSE for validation set:', mean_squared_error(y_val, y_pred, squared=False))\n",
    "print('\\nRMSE for log of validation set:', mean_squared_error(np.log(y_val), np.log(y_pred), squared=False))\n",
    "\n",
    "\n",
    "print('Random Forest Model done')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509f1821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.ensemble import GradientBoostingRegressor\\n\\nparam_grid = {\\n    \\'learning_rate\\': np.arange(0.01, 1.01, 0.01),\\n    \\'n_estimators\\': range(100, 300, 1),\\n    \\'max_depth\\': range(1, 300, 1),\\n    \\'min_samples_split\\': np.arange(0.01, 1.0, 0.01),\\n    \\'min_samples_leaf\\': range(1, 300, 1),\\n    \\'max_leaf_nodes\\': range(2, 100, 1),\\n    \\'min_impurity_decrease\\': np.arange(0.01, 1.01, 0.01)\\n}\\n\\nfor i in range(5):\\n    random_search_GBR = RandomizedSearchCV(estimator = GradientBoostingRegressor(random_state=0), \\n                                 param_distributions = param_grid, cv = 5, n_jobs = -1,\\n                                 verbose = 2, scoring=RMSLE, error_score=\\'raise\\')\\n\\n    random_search_GBR.fit(X_train, y_train)\\n\\n    print(f\\'Randomized Search Results {i}:\\')\\n    print(\\'Best parameters:\\', random_search_GBR.best_params_)\\n    print(\"Best RMSLE: \", random_search_GBR.best_score_, \\'\\n\\n\\')\\n\\n# If chosen as final model\\n\\np = random_search_GBR.best_params_\\n\\nbest_model_GBR = GradientBoostingRegressor(learning_rate=p[\\'learning_rate\\'], n_estimators=p[\\'n_estimators\\'],\\n                                       max_depth=p[\\'max_depth\\'], min_samples_split=p[\\'min_samples_split\\'],\\n                                       min_samples_leaf=p[\\'min_samples_leaf\\'],\\n                                       max_leaf_nodes=p[\\'max_leaf_nodes\\'],\\n                                       min_impurity_decrease=p[\\'min_impurity_decrease\\'])\\n\\nbest_model_GBR.fit(X_train, y_train)\\n\\ny_pred = best_model_GBR.predict(X_val)\\n\\nprint(\\'\\nRMSE for validation set:\\', mean_squared_error(y_val, y_pred, squared=False))\\nprint(\\'\\nRMSE for log of validation set:\\', mean_squared_error(np.log(y_val), np.log(y_pred), squared=False))\\n\\nprint(\\'GradientBoosting Model done\\')\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingRegressor\n",
    "'''\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.01),\n",
    "    'n_estimators': range(100, 300, 1),\n",
    "    'max_depth': range(1, 300, 1),\n",
    "    'min_samples_split': np.arange(0.01, 1.0, 0.01),\n",
    "    'min_samples_leaf': range(1, 300, 1),\n",
    "    'max_leaf_nodes': range(2, 100, 1),\n",
    "    'min_impurity_decrease': np.arange(0.01, 1.01, 0.01)\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    random_search_GBR = RandomizedSearchCV(estimator = GradientBoostingRegressor(random_state=0), \n",
    "                                 param_distributions = param_grid, cv = 5, n_jobs = -1,\n",
    "                                 verbose = 2, scoring=RMSLE, error_score='raise')\n",
    "\n",
    "    random_search_GBR.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Randomized Search Results {i}:')\n",
    "    print('Best parameters:', random_search_GBR.best_params_)\n",
    "    print(\"Best RMSLE: \", random_search_GBR.best_score_, '\\n\\n')\n",
    "\n",
    "# If chosen as final model\n",
    "\n",
    "p = random_search_GBR.best_params_\n",
    "\n",
    "best_model_GBR = GradientBoostingRegressor(learning_rate=p['learning_rate'], n_estimators=p['n_estimators'],\n",
    "                                       max_depth=p['max_depth'], min_samples_split=p['min_samples_split'],\n",
    "                                       min_samples_leaf=p['min_samples_leaf'],\n",
    "                                       max_leaf_nodes=p['max_leaf_nodes'],\n",
    "                                       min_impurity_decrease=p['min_impurity_decrease'])\n",
    "\n",
    "best_model_GBR.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model_GBR.predict(X_val)\n",
    "\n",
    "print('\\nRMSE for validation set:', mean_squared_error(y_val, y_pred, squared=False))\n",
    "print('\\nRMSE for log of validation set:', mean_squared_error(np.log(y_val), np.log(y_pred), squared=False))\n",
    "\n",
    "print('GradientBoosting Model done')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b273d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport keras\\nimport tensorflow as tf\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Dropout, Flatten\\nfrom keras.layers import Conv1D, MaxPooling1D\\nimport keras.optimizers\\nimport warnings\\nfrom keras.utils import np_utils\\nfrom keras.callbacks import EarlyStopping\\nimport keras.regularizers\\nwarnings.filterwarnings(\"ignore\")\\nfrom keras.wrappers.scikit_learn import KerasRegressor\\nfrom keras import backend as K\\n\\n# Old Architectures Used\\n\\n#model = Sequential()\\n#model.add(Dense(233, input_dim=233,\\n#             activation=\\'relu\\'))\\n#model.add(Dropout(0.1)) <= also tried w/o dropout\\n#model.add(Dense(233, activation=\\'relu\\'))\\n#model.add(Dense(1))\\n\\n\\n# I learned about the keras backend and used the code provided by users Germán Sanchis and Eric Aya \\n# from https://stackoverflow.com/questions/43855162/rmse-rmsle-loss-function-in-keras\\n# to make the rmsle function right below\\n\\ndef rmsle(y_true, y_pred):\\n    return K.sqrt(K.mean(K.square(K.log(y_pred + 1) - K.log(y_true + 1))))\\n\\n\\n# Building Neural Network\\ndef build_model():\\n    model = Sequential()\\n    model.add(Dense(233, input_dim=233,\\n                 activation=\\'relu\\'))\\n    model.add(Dense(233, activation=\\'relu\\'))\\n    model.add(Dense(1))\\n    \\n    model.compile(loss=\\'mean_squared_error\\',\\n              optimizer=\\'adam\\',\\n              metrics=rmsle)\\n    \\n    return model\\n\\n# Early Stopping\\nstop_early = EarlyStopping(monitor=\\'val_rmsle\\', patience=2, verbose=0)\\n\\nparam_grid = {\\n    \\'batch_size\\': range(100, 300, 1),\\n}\\n\\n# I got guidance from Jason Brownlee\\'s article on\\n# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\\n# to set up the Keras neural network to work with Scikit Learn.\\n\\nmodel_NN = KerasRegressor(build_fn=lambda: build_model(), epochs=100, \\n                          callbacks=[stop_early], shuffle=True, validation_split=0.1, verbose=1)\\n\\nrandom_search_NN = RandomizedSearchCV(estimator = model_NN, \\n                                 param_distributions = param_grid, cv = 5, n_jobs = -1,\\n                                 verbose = 1, scoring=RMSLE, error_score=\\'raise\\')\\n\\nrs_results = random_search_NN.fit(X_train, y_train)\\n\\nprint(\\'Randomized Search Results:\\')\\nprint(\"Best parameters:\", rs_results.best_params_)\\nprint(\"Best RMSLE: \", rs_results.best_score_)\\n\\n# If chosen as final model\\n\\np = random_search_NN.best_params_\\n\\nmodel = build_model()\\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=p[\\'batch_size\\'],\\n          epochs=300, shuffle=True, callbacks=[stop_early])\\n\\ny_pred = model.predict(X_val)\\n\\nprint(\\'\\nRMSE for validation set:\\', mean_squared_error(y_val, y_pred, squared=False))\\nprint(\\'\\nRMSE for log of validation set:\\', mean_squared_error(np.log(y_val), np.log(y_pred), squared=False))\\n\\nprint(\\'\\n Neural Network Model done\\')\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D Convolutional Neural Network\n",
    "'''\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "import keras.optimizers\n",
    "import warnings\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.regularizers\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "\n",
    "# Old Architectures Used\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Dense(233, input_dim=233,\n",
    "#             activation='relu'))\n",
    "#model.add(Dropout(0.1)) <= also tried w/o dropout\n",
    "#model.add(Dense(233, activation='relu'))\n",
    "#model.add(Dense(1))\n",
    "\n",
    "\n",
    "# I learned about the keras backend and used the code provided by users Germán Sanchis and Eric Aya \n",
    "# from https://stackoverflow.com/questions/43855162/rmse-rmsle-loss-function-in-keras\n",
    "# to make the rmsle function right below\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.log(y_pred + 1) - K.log(y_true + 1))))\n",
    "\n",
    "\n",
    "# Building Neural Network\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(233, input_dim=233,\n",
    "                 activation='relu'))\n",
    "    model.add(Dense(233, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=rmsle)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Early Stopping\n",
    "stop_early = EarlyStopping(monitor='val_rmsle', patience=2, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': range(100, 300, 1),\n",
    "}\n",
    "\n",
    "# I got guidance from Jason Brownlee's article on\n",
    "# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "# to set up the Keras neural network to work with Scikit Learn.\n",
    "\n",
    "model_NN = KerasRegressor(build_fn=lambda: build_model(), epochs=100, \n",
    "                          callbacks=[stop_early], shuffle=True, validation_split=0.1, verbose=1)\n",
    "\n",
    "random_search_NN = RandomizedSearchCV(estimator = model_NN, \n",
    "                                 param_distributions = param_grid, cv = 5, n_jobs = -1,\n",
    "                                 verbose = 1, scoring=RMSLE, error_score='raise')\n",
    "\n",
    "rs_results = random_search_NN.fit(X_train, y_train)\n",
    "\n",
    "print('Randomized Search Results:')\n",
    "print(\"Best parameters:\", rs_results.best_params_)\n",
    "print(\"Best RMSLE: \", rs_results.best_score_)\n",
    "\n",
    "# If chosen as final model\n",
    "\n",
    "p = random_search_NN.best_params_\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=p['batch_size'],\n",
    "          epochs=300, shuffle=True, callbacks=[stop_early])\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print('\\nRMSE for validation set:', mean_squared_error(y_val, y_pred, squared=False))\n",
    "print('\\nRMSE for log of validation set:', mean_squared_error(np.log(y_val), np.log(y_pred), squared=False))\n",
    "\n",
    "print('\\n Neural Network Model done')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce9e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Search Results 0:\n",
      "Best parameters: {'subsample': 0.33, 'n_estimators': 711, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.065, 'lambda': 19, 'gamma': 49, 'colsample_bytree': 0.23, 'alpha': 17}\n",
      "Best RMSLE:  0.12498681689303856 \n",
      "\n",
      "\n",
      "XGBoost Model done\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "'''\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.05),\n",
    "    'n_estimators': range(1, 1000, 50),\n",
    "    'max_depth': range(1, 50, 1),\n",
    "    'colsample_bytree': np.arange(0.01, 1.0, 0.1),\n",
    "    'subsample': np.arange(0.01, 1.01, 0.1),\n",
    "    'alpha': range(0, 100, 5),\n",
    "    'lambda': range(0, 100, 5),\n",
    "    'gamma': range(0, 100, 5)\n",
    "}\n",
    "'''\n",
    "\n",
    "# Current best\n",
    "'''\n",
    "Best parameters: {'subsample': 0.33,\n",
    "'n_estimators': 711, 'max_depth': 5,\n",
    "'learning_rate': 0.08, 'lambda': 19,\n",
    "'gamma': 40, 'colsample_bytree': 0.23,\n",
    "'alpha': 17}\n",
    "'''\n",
    "\n",
    "# 0.1262054666438511 \n",
    "param_grid = {\n",
    "    'learning_rate': [0.065],\n",
    "    'n_estimators': [711],\n",
    "    'max_depth': [5],\n",
    "    'min_child_weight': [1],\n",
    "    'colsample_bytree': [0.23],\n",
    "    'subsample': [0.33],\n",
    "    'alpha': [17],\n",
    "    'lambda': [19],\n",
    "    'gamma': [49],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    # Grid Search\n",
    "    #random_search_XGB = GridSearchCV(estimator = XGBRegressor(seed=0), \n",
    "    #                             param_grid = param_grid, cv = 5, n_jobs = -1,\n",
    "    #                             verbose = 2, scoring=RMSLE, error_score='raise')\n",
    "    # Random Search\n",
    "\n",
    "    random_search_XGB = RandomizedSearchCV(estimator = XGBRegressor(seed=0), \n",
    "                                 param_distributions = param_grid, cv = 5, n_jobs = -1,\n",
    "                                 verbose = 2, scoring=RMSLE, error_score='raise')\n",
    "    \n",
    "    random_search_XGB.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Randomized Search Results {i}:')\n",
    "    # From https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n",
    "    print(\"Best parameters:\", random_search_XGB.best_params_)\n",
    "    print(\"Best RMSLE: \", random_search_XGB.best_score_, '\\n\\n')\n",
    "\n",
    "\n",
    "# If chosen as final model\n",
    "\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "\n",
    "p = random_search_XGB.best_params_\n",
    "\n",
    "best_model_XGB = xgb.XGBRegressor(subsample=0.33, n_estimators=711, max_depth=5, learning_rate=0.08,\n",
    "                      reg_lambda=19, gamma=40, colsample_bytree=0.23, alpha=17)\n",
    "\n",
    "best_model_XGB.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model_XGB.predict(X_val)\n",
    "\n",
    "print('\\nRMSE for validation set:', mean_squared_error(y_val, y_pred, squared=False))\n",
    "print('\\nRMSE for log of validation set:', mean_squared_error(np.log(y_val), np.log(y_pred), squared=False))\n",
    "'''\n",
    "\n",
    "print('XGBoost Model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a6f949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ypred 0.9894072449977998\n",
      "1459\n"
     ]
    }
   ],
   "source": [
    "# Final Model\n",
    "\n",
    "model = xgb.XGBRegressor(subsample=0.33, n_estimators=711, max_depth=5, min_child_weight=1, learning_rate=0.08,\n",
    "                      reg_lambda=19, gamma=49, colsample_bytree=0.23, alpha=17)\n",
    "model.fit(X_train, y_train)\n",
    "ypred = model.predict(X_test)\n",
    "print('ypred', model.score(X_train, y_train))\n",
    "print(len(ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a112c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file.\n",
    "submission = pd.DataFrame(ypred, columns=['SalePrice']) # Create new dataframe.\n",
    "submission['Id'] = df_test['Id'].astype('Int32') # Kaggle expects two columns: Id, SalePrice.\n",
    "submission.to_csv('LR_submission6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05ea4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
